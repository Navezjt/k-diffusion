{
  "model": {
    "type": "image_transformer_v2",
    "input_channels": 4,
    // this becomes 512x512px
    "input_size": [64, 64],
    // deliberately reduced patch size (from 4 -> 2) compared to RGB
    "patch_size": [2, 2],
    "depths": [2, 2, 16],
    // for latent: we might need to increase base width compared to RGB's 128
    // or maybe not? because we reduced the patch size
    "widths": [384, 768, 1536],
    "loss_config": "karras",
    // latents want snr, not soft-min-snr
    "loss_weighting": "snr",
    "loss_scales": 1,
    "dropout_rate": [0.0, 0.0, 0.0],
    "mapping_dropout_rate": 0.0,
    // for latent datasets, only horizontal flip augmentations are enabled. if you want other augmentations:
    // you'd have to precompute that into the dataset via imagenet_vae_loading.py
    "augment_prob": 0.5,
    "sigma_data": 0.5,
    "sigma_min": 1e-2,
    // reduced sigma_max (160 -> 80) since canvas size is smaller than the 256px that this config came from.
    // we might even be able to halve it again to 40.
    "sigma_max": 80,
    "sigma_sample_density": {
      "type": "cosine-interpolated"
    }
  },
  "dataset": {
    "type": "wds-class",
    "latents": true,
    // !! fill location in yourself (including the number of .tar files) !!
    "location": "/fsx/home-tmabraham/datasets/imagenet-latents-512/wds/{00000..00069}.tar",
    "wds_latent_key": "latent.pth",
    "class_cond_key": "cls.txt",
    "num_classes": 1000,
    "classes_to_captions": "imagenet-1k",
    "estimated_samples": 1281167,
    // average obtained after SDXL VAE-encoding all of imagenet-1k at 512px
    // torch.load('val.pt', weights_only=True).tolist()
    "channel_means": [-0.614419162273407, 0.7538465261459351, 0.13934417068958282, 1.8618509769439697],
    // torch.load('sq.pt', weights_only=True).tolist()
    "channel_squares": [68.558349609375, 36.764495849609375, 47.566280364990234, 33.02168655395508]
    // std is:
    // torch.sqrt(squares - means**2)
    // [8.257168769836426, 6.016328811645508, 6.895423412322998, 5.436469078063965]
    // its reciprocal is actually close to SDXL VAE's scaling_factor of 0.13025
    // [0.12110688537359238, 0.16621431708335876, 0.14502373337745667, 0.18394291400909424]
  },
  "optimizer": {
    "type": "adamw",
    "lr": 5e-4,
    "betas": [0.9, 0.95],
    "eps": 1e-8,
    "weight_decay": 1e-2
  },
  "lr_sched": {
    "type": "constant"
  },
  "ema_sched": {
    "type": "inverse",
    "power": 0.75,
    "max_value": 0.9999
  }
}
